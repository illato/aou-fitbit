{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Workspace and Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas\n",
    "from datetime import datetime\n",
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the bucket name\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "# List objects in the bucket\n",
    "print(subprocess.check_output(f\"gsutil ls -r {my_bucket}\", shell=True).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Create function to set seeds for reproducibility\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(offset=0):\n",
    "    import os\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # Set the seed for numpy\n",
    "    np.random.seed(42+offset)\n",
    "\n",
    "    # Set the seed for the Python random module\n",
    "    random.seed(42+offset)\n",
    "\n",
    "    # Set the seed for TensorFlow\n",
    "    tf.random.set_seed(42+offset)\n",
    "\n",
    "    # Ensure reproducibility with certain environment variables\n",
    "    os.environ['PYTHONHASHSEED'] = str(42+offset)\n",
    "\n",
    "\n",
    "    ### Hold off on more extensive seeds (below) until verified necessary\n",
    "\n",
    "\n",
    "    # # Configure TensorFlow to use a single thread if required\n",
    "    # tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    # tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "    # # Optionally, set environment variables to control NumPy threading behavior\n",
    "    # os.environ['OMP_NUM_THREADS'] = '1'\n",
    "    # os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "    # # Example to demonstrate reproducibility\n",
    "    # print(\"Numpy Random:\", np.random.rand(3))\n",
    "    # print(\"Python Random:\", random.random())\n",
    "\n",
    "    # # TensorFlow example\n",
    "    # tf_example = tf.random.uniform([3])\n",
    "    # print(\"TensorFlow Random:\", tf_example)\n",
    "\n",
    "    # # PyTorch Example (if using PyTorch)\n",
    "    # import torch\n",
    "\n",
    "    # torch.manual_seed(42+offset)\n",
    "    # if torch.cuda.is_available():\n",
    "    #     torch.cuda.manual_seed(42+offset)\n",
    "    #     torch.cuda.manual_seed_all(42+offset)  # if using multi-GPU.\n",
    "    #     torch.backends.cudnn.deterministic = True  # cuDNN\n",
    "    #     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # # Generate reproducible random numbers with PyTorch\n",
    "    # print(\"PyTorch Random:\", torch.rand(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Create function to start/stop logging RAM usage to file\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "import threading\n",
    "import time\n",
    "from google.cloud import storage\n",
    "\n",
    "def log_memory_usage(stop_event, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        while not stop_event.is_set():\n",
    "            # Log memory usage to a local file\n",
    "            memory_info = psutil.virtual_memory()\n",
    "            gb_used = memory_info.used / (1024 ** 3)\n",
    "            mem_usage = f\"{time.ctime()}: {gb_used:.2f} GB\\n\"\n",
    "            print(mem_usage)\n",
    "            f.write(mem_usage)\n",
    "            f.flush()\n",
    "            \n",
    "            # Upload the local file to GCS\n",
    "            try:\n",
    "                destination_blob_name = f'logs/{file_name}'\n",
    "                upload_to_gcs(file_name, destination_blob_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to upload to GCS: {e}\")\n",
    "                \n",
    "            time.sleep(30)\n",
    "            \n",
    "def upload_to_gcs(source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # Get the bucket name\n",
    "    my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "    # Initialize a storage client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(my_bucket[5:])\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Upload the file\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "#     print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_event = threading.Event()\n",
    "memory_thread = None\n",
    "thread_lock = threading.Lock()  # To ensure thread-safe operations\n",
    "\n",
    "def RAM_start():\n",
    "    global stop_event\n",
    "    global memory_thread\n",
    "\n",
    "    with thread_lock:\n",
    "        # Clear the stop event if it is set\n",
    "        if stop_event.is_set():\n",
    "            stop_event.clear()\n",
    "\n",
    "        file_name = 'memory_usage.txt'\n",
    "        \n",
    "        # Stop the existing thread if it is running\n",
    "        if memory_thread and memory_thread.is_alive():\n",
    "            RAM_stop()\n",
    "        \n",
    "        # Create and start a new memory logging thread\n",
    "        memory_thread = threading.Thread(target=log_memory_usage, args=(stop_event, file_name))\n",
    "        memory_thread.start()\n",
    "        print(\"Memory logging started\")\n",
    "\n",
    "def RAM_stop():\n",
    "    global stop_event\n",
    "    global memory_thread\n",
    "\n",
    "    with thread_lock:\n",
    "        # Set the stop event to signal the thread to stop\n",
    "        stop_event.set()\n",
    "\n",
    "        # Wait for the thread to finish if it exists\n",
    "        if memory_thread:\n",
    "            memory_thread.join()\n",
    "            memory_thread = None  # Reset the thread to None\n",
    "            print(\"Memory logging stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAM_start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read `daily_df` from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp {my_bucket}/data/dfs/daily_df_v2.csv daily_df_v2.csv\n",
    "!gsutil cp {my_bucket}/data/dfs/daily_df_labels_v2.csv daily_df_labels_v2.csv\n",
    "!gsutil cp {my_bucket}/data/dfs/demographics_df.csv demographics_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read Demographics CSV\n",
    "# demo_df = pd.read_csv(f\"{my_bucket}/data/dfs/demographics_df.csv\")\n",
    "demo_df = pd.read_csv(f\"demographics_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_df = pd.read_csv(f\"{my_bucket}/data/dfs/daily_df.csv\")\n",
    "# daily_df = pd.read_csv(f\"{my_bucket}/data/dfs/daily_df_v2.csv\")\n",
    "daily_df = pd.read_csv(f\"daily_df_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = pd.read_csv(f\"{my_bucket}/data/dfs/daily_df_labels_v2.csv\", index_col=0)\n",
    "labels = pd.read_csv(f\"daily_df_labels_v2.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seeds for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove naps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove naps and only keep rows for main sleep__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep data for main sleep, not naps\n",
    "daily_df = daily_df[daily_df['is_main_sleep']]\n",
    "\n",
    "# remove is_main_sleep column\n",
    "daily_df = daily_df.drop(columns=['is_main_sleep'])\n",
    "daily_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check % 0's and remove columns if appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_zeros(df):\n",
    "    # Calculate the percentage of zeros in each column\n",
    "    zero_percent = (df == 0).mean() * 100\n",
    "\n",
    "    # Print the percentage of zero values in each column\n",
    "    for column, percentage in zero_percent.items():\n",
    "        if percentage > 0:\n",
    "            print(f\"Column {column:>25}: {percentage:8.4f}% zeros\")\n",
    "            \n",
    "perc_zeros(daily_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Consider removing `minute_after_wakeup` (leaving for now)\n",
    "\n",
    "`minute_after_wakeup`: The total number of minutes after the user woke up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "daily_df.minute_after_wakeup.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in [75,76,80,85,90,95,99]:\n",
    "    print(f'minute_after_wakeup {i} percentile: {np.percentile(daily_df.minute_after_wakeup, q=i):4.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "leaving `minute_after_wakeup` for now\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(daily_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "daily_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### __If there are NaN values that make sense to fill with 0's, replace__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def perc_nan(df):\n",
    "    # Calculate the percentage of NaN values in each column\n",
    "    nan_percent = df.isna().mean() * 100\n",
    "\n",
    "    # Print the percentage of NaN values in each column\n",
    "    for column, percentage in nan_percent.items():\n",
    "        if percentage > 0:\n",
    "            print(f\"Column {column:>18}: {percentage:8.4f}% NaN\")\n",
    "            \n",
    "perc_nan(daily_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Drop `minute_restless` per ~70% missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "daily_df = daily_df.drop(columns=['minute_restless'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perc_nan(daily_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### ~~Fill `calorie_count` and `minute_in_zone` with 0's~~\n",
    "\n",
    "All of Us Controlled Tier Dataset v7 CDR Data Dictionary (C2022Q4R9)\n",
    "\n",
    "`calorie_count`: Number calories burned within the custom heart rate zone.\n",
    "\n",
    "`minute_in_zone`: Number minutes within the specified heart rate zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# daily_df.loc[:, ['calorie_count','minute_in_zone']] = daily_df.loc[:, ['calorie_count','minute_in_zone']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# perc_nan(daily_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Drop `elevation` \n",
    "\n",
    "__per don't have `device.src_id` to determine unit of measure (using `floors` per unified measure)__  \n",
    "\n",
    "\n",
    "All of Us Controlled Tier Dataset v7 CDR Data Dictionary (C2022Q4R9)\n",
    "\n",
    "`elevation`: The elevation traveled for the day displayed in the units defined by the data source. When __src_id__ is __PTSC__, the unit is feet. When __src_id__ is __TPC__, the unit is meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "daily_df = daily_df.drop(columns=['elevation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fill `floors` with 0's\n",
    "All of Us Controlled Tier Dataset v7 CDR Data Dictionary (C2022Q4R9)\n",
    "\n",
    "`floors`: The floors provides ONLY the count of how many floors the Fitbit device counted as the user ascended in elevation. The researcher can determine how many \"feet\" or \"meters\" the user climbed as the device determines a floor every time the user ascends 10 feet (3 meters). Essentially 1 Floor = 10 Feet (3 Meters).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "daily_df.loc[:, ['floors']] = daily_df.loc[:, ['floors']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perc_nan(daily_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "~~### __Fill `minute_`* for sleep w/0; `floors` w/0__~~\n",
    "### __Drop `'minute_deep', 'minute_light','minute_rem', 'minute_wake'` per ~29% missing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Replace NaN values in those columns with 0 and reassign using .loc\n",
    "# daily_df.loc[:, ['minute_deep', 'minute_light','minute_rem', 'minute_wake']] = daily_df.loc[:, \n",
    "#                 ['minute_deep', 'minute_light', 'minute_rem', 'minute_wake']].fillna(0)\n",
    "\n",
    "daily_df = daily_df.drop(columns=['minute_deep', 'minute_light', 'minute_rem', 'minute_wake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perc_nan(daily_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns_to_check = ['std_hr', 'morning_hr', 'afternoon_hr', 'evening_hr', 'night_hr']\n",
    "nan_person_ids = daily_df[daily_df[columns_to_check].isna().any(axis=1)]['person_id'].unique()\n",
    "print(len(nan_person_ids)/len(daily_df.person_id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "98% have missing values for at least one of these  \n",
    "\n",
    "will simply handle in `get_random_chunk` by making multiple attempts to get a 10-day span w/out NaN's\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(daily_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(demo_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Prepared (unchunked) Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = daily_df.sort_values(['person_id','date'])\n",
    "daily_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df.to_csv(f\"daily_df_v2_prepped.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp daily_df_v2_prepped.csv {my_bucket}/data/dfs/daily_df_v2_prepped.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAM_stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
